# image classification notes

#### 一些重点总结 

* 中文笔记链接：https://zhuanlan.zhihu.com/p/20894041?refer=intelligentunit 

* 主要是介绍`kNN分类器`。但是注意：`kNN分类器`可以作为图像分类的一个上手算法，但是在实际中几乎不会使用`kNN`来完成`图像分类`。
* 例子中使用`5-NN`来完成二维上数据点的分类时，`白色区域`表示落在该区域中的点不能明确分类到哪一类。
    * 例如，该点可能和`2个绿色点`最近，同时和`2个红色点`最近，这时候使用`投票规则`来分类时，就不能确定将该点分到哪一类中。
    * `K` 的值越大，决策边界越平滑，避免了一些`噪音点`的出现
* 永远不要用测试集来对算法进行调优。调优的工作永远是验证集需要做的。
* 测试集只在找到最优的参数之后，在模型上运行一次，并将这个分类的结果(分类准确率)作为算法最后的性能。
* 交叉验证可以在选择最优超参数时，进一步地避免噪音。
* `kNN分类器`更多是根据**图片颜色背景和颜色**来分类图片的，而不是基于图片的语义信息
* 交叉验证的过程中，训练集被划分的份数越多，那么分类器的效果越好，当然耗费的计算资源也就更多，是因为每一份数据都需要作为验证集来验证算法的效果，如果份数越多，则对于同一个超参数，算法执行的次数也就越多。
* 在用交叉验证得到最优参数后，不要在将`训练集+验证集`合并作为一个更大的训练集，再来对模型进行训练，因为这样会破坏最优参数的估计。直接使用这个最优的参数来在测试集上面测试，将测试集上面的结果作为算法在该数据上面的表现。     

#### 一些需要花时间完成的事情  


- [ ] `CIFAR-10`数据集官网：http://www.cs.toronto.edu/~kriz/cifar.html 
- [ ] `Kaggle算法竞赛`基于`cifar-10`的排行榜：https://www.kaggle.com/c/cifar-10/leaderboard 
- [ ] 关于`L1-distance`和`L2-distance`之间的联系：https://planetmath.org/vectorpnorm 
- [ ] 使用`FLANN`来加速 nearest neighbor 在数据上面的计算速度。一般包含 `KD树` `K-means`等预处理步骤。http://www.cs.ubc.ca/research/flann/  
- [ ] 使用 `t-SNE` 可视化技术来对图像进行二维排列。http://lvdmaaten.github.io/tsne/ 
- [ ] 使用`随机投影`来完成数据降维
- [ ] 论文`A few useful things to know about machine learning`  https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf 
